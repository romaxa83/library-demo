services:

  nginx:
    image: nginx:alpine
    container_name: ${APP_NAME}__nginx
    restart: unless-stopped
    build:
      context: .
      dockerfile: infrastructures/nginx/Dockerfile
    ports:
      - 80:80
    volumes:
      - ./:/app
      - ./public:/public:ro
      - ./logs/nginx:/var/log/nginx
    working_dir: /app
    healthcheck:
      # Проверяем доступность страницы stub_status
      test: [ "CMD-SHELL", "wget -qO- http://127.0.0.1/stub_status || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  app:
    container_name: ${APP_NAME}__app
    build:
      context: .
      dockerfile: infrastructures/app/Dockerfile
    working_dir: /app
    volumes:
      - ./:/app/
    environment:
      - DB_HOST=db
      - REDIS_HOST=redis
      - RABBITMQ_HOST=rabbitmq
    # Если используете FastAPI, переопределяем команду для авторелоада
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
    depends_on:
      db:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy

  db:
    image: &db-image postgres:18-alpine
    container_name: ${APP_NAME}__db
    ports:
      - ${DB_PORT}:${DB_PORT}
    environment:
      POSTGRES_DB: ${DB_DATABASE}
      POSTGRES_USER: ${DB_USERNAME}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - db_data:/var/lib/postgresql/data/
      - ./logs/postgresql:/var/log/postgresql
      - ./infrastructures/postgresql/init-db:/docker-entrypoint-initdb.d
      - ./infrastructures/postgresql/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: pg_isready -U ${DB_USERNAME} -d ${DB_DATABASE}
      interval: 5s
      timeout: 10s
      retries: 20

  db_test:
    image: *db-image
    container_name: ${APP_NAME}__db_test
    restart: unless-stopped
    # розширюємо кількість одночасних підключень до Postgres БД та кількість транзакцій. Важливо для комфортного запуску паралельних тестів.
    command: postgres -c 'max_connections=250' -c 'max_locks_per_transaction=128'
    ports:
      - 5433:5432
    environment:
      POSTGRES_DB: db_test
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - /var/lib/postgresql/data

  rabbitmq:
    image: rabbitmq:4.1-management
    container_name: ${APP_NAME}__rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
    ports:
      - "5672:5672"
      - "15672:15672"
      - "15692:15692" # Порт для Prometheus
    # Команда для включения плагина (для прометеуса) при старте
    command: >
      sh -c "rabbitmq-plugins enable rabbitmq_prometheus && rabbitmq-server"
    healthcheck:
      # Проверяет, готов ли узел принимать подключения
      test: [ "CMD", "rabbitmq-diagnostics", "-q", "check_port_connectivity" ]
      interval: 10s     # Как часто проверять
      timeout: 5s       # Сколько ждать ответа
      retries: 5        # Сколько попыток до статуса "unhealthy"
      start_period: 15s # Время на "раскачку" после старта
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - ./infrastructures/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro

  redis:
    image: redis:alpine
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    container_name: ${APP_NAME}__redis
    ports:
      - ${REDIS_PORT}:${REDIS_PORT}
    volumes:
      - redis_data:/data
#    command: redis-server --requirepass ${REDIS_PASSWORD} # раскомментировать если нужно чтоб пароль был обязательным
    healthcheck:
      test: redis-cli ping
      interval: 5s
      timeout: 10s
      retries: 20

  redis_gui:
    image: redis/redisinsight
    container_name: ${APP_NAME}__redis_gui
    ports:
      - 5540:5540

  mailer:
    image: axllent/mailpit:latest
    container_name: ${APP_NAME}__mailer
    hostname: ${APP_NAME}__mailer
    ports:
      - 8025:8025
      - 1025:1025

  prometheus:
    image: prom/prometheus:v3.2.1
    container_name: ${APP_NAME}__prometheus
    networks:
      default:
        aliases:
          - prometheus
    volumes:
      - prometheus_data:/prometheus
      - ./infrastructures/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  grafana:
    image: grafana/grafana:11.6.0
    container_name: ${APP_NAME}__grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infrastructures/grafana/provisioning:/etc/grafana/provisioning
    env_file:
      - .env
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_ADMIN_PASSWORD}
    ports:
      - "3000:3000"
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s

  nginx_exporter:
    image: nginx/nginx-prometheus-exporter:latest
    container_name: ${APP_NAME}__nginx_exporter
    #    ports:
    #     - "9113:9113"
    command:
      - '-nginx.scrape-uri=http://nginx/stub_status'
    restart: always
    depends_on:
      nginx:
        condition: service_healthy
    healthcheck:
      # Проверяем, что порт 9113 отвечает
      test: [ "CMD", "/usr/bin/nginx-prometheus-exporter", "-version" ] # Простая проверка бинарника
      # Или более надежно через проверку порта (если в образе есть nc или curl):
      # test: ["CMD-SHELL", "nc -z localhost 9113 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3

  node_exporter:
    image: prom/node-exporter:latest
    container_name: ${APP_NAME}__node_exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
#    ports:
#      - "9100:9100"
    healthcheck:
      test: [ "CMD-SHELL", "nc -z localhost 9100 || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 3

  postgres_exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: ${APP_NAME}__postgres_exporter
    depends_on:
      db:
        condition: service_healthy
    env_file:
      - .env
    environment:
      DATA_SOURCE_NAME: postgresql://${DB_USERNAME}:${DB_PASSWORD}@db:${DB_PORT}/postgres?sslmode=disable
#    ports:
#      - "9187:9187"
    restart: always
    healthcheck:
      test: [ "CMD-SHELL", "nc -z localhost 9187 || exit 1" ]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s

#  logstash:
#    image: docker.elastic.co/logstash/logstash:8.17.3
#    container_name: logstash
#    volumes:
#      - ./infrastructures/elk/logstash/pipeline:/usr/share/logstash/pipeline
#      - ./infrastructures/elk/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
#    env_file:
#      - .env
#    environment:
#        - LS_JAVA_OPTS=${LS_JAVA_OPTS}
#    ports:
#      - "5044:5044"
#    healthcheck:
#      test: [ "CMD-SHELL", "curl -s -XGET http://localhost:9600" ]
#      interval: 15s
#      timeout: 5s
#      retries: 5
#    depends_on:
#      elasticsearch:
#        condition: service_healthy

#  elasticsearch:
#    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.3
#    container_name: elasticsearch
#    volumes:
#      - elastic_data:/usr/share/elasticsearch/data
#    env_file:
#      - .env
#    environment:
#      - discovery.type=${ES_DISCOVERY_TYPE}
#      - xpack.security.enabled=${ES_XPACK_SECURITY}  # Отключаем авторизацию для локальной разработки
#      - ES_JAVA_OPTS=${ES_JAVA_OPTS}  # Ограничение памяти
#    ports:
#      - "9200:9200"
#    deploy:
#      resources:
#        limits:
#          memory: 2G
#    healthcheck:
#      test: [ "CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"\\|\"status\":\"yellow\"'" ]
#      interval: 10s
#      timeout: 5s
#      retries: 5

#  kibana:
#    image: docker.elastic.co/kibana/kibana:8.17.3
#    container_name: kibana
#    env_file:
#      - .env
#    ports:
#      - "5601:5601"
#    healthcheck:
#      test: [ "CMD-SHELL", "curl -s -I http://localhost:5601 | grep -q '200 OK'" ]
#      interval: 15s
#      timeout: 5s
#      retries: 5
#    depends_on:
#      elasticsearch:
#        condition: service_healthy

#  filebeat:
#    build: ./infrastructures/elk/filebeat
#    depends_on:
#      - app
#      - logstash
#      - kibana
#    volumes:
#      - ./logs/app:/app/logs:ro  # Папка с логами приложения
#      - ./logs/nginx:/var/log/nginx:ro  # Логи nginx
#      - ./logs/postgresql:/var/log/postgresql:ro
#      - filebeat_registry:/usr/share/filebeat/data
#    env_file:
#      - .env
#    restart: on-failure

volumes:
  db_data:
    driver: local
    name: ${APP_NAME}_db_data
  redis_data:
    driver: local
    name: ${APP_NAME}_redis_data
  rabbitmq_data:
    driver: local
    name: ${APP_NAME}_rabbitmq_data
  grafana_data:
    driver: local
    name: ${APP_NAME}_grafana_data
  prometheus_data:
    driver: local
    name: ${APP_NAME}_prometheus_data
#  elastic_data:
#    driver: local
#    name: ${APP_NAME}_elastic_data
#  filebeat_registry: